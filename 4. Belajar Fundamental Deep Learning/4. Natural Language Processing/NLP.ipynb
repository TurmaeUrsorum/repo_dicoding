{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f649f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', 'clang 18'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.5.1'), ('cudnn_version', '9'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.sysconfig.get_build_info())\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503849d",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cca45",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd34ec",
   "metadata": {},
   "source": [
    "Case folding adalah langkah sederhana pada pra-pemrosesan teks yang bertujuan untuk mengubah semua huruf dalam dokumen teks menjadi huruf kecil. Tujuannya adalah membuat teks lebih seragam dan memudahkan proses analisis teks, terutama dalam pengenalan kata-kata yang sama meskipun berbeda penulisan huruf besar-kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825fbe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase.\n",
      "Teks setelah diubah menjadi lowercase: ini adalah contoh teks yang akan dikonversi menjadi lowercase.\n"
     ]
    }
   ],
   "source": [
    "# Contoh teks\n",
    "teks_asli = \"Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase.\"\n",
    " \n",
    "# Mengubah teks menjadi lowercase\n",
    "teks_lowercase = teks_asli.lower()\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\", teks_asli)\n",
    "print(\"Teks setelah diubah menjadi lowercase:\", teks_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bff86b",
   "metadata": {},
   "source": [
    "## Removal Special Characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb44d87",
   "metadata": {},
   "source": [
    "### Menghapus Angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4924275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks dengan angka: Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\n",
      "Teks tanpa angka: Ini adalah contoh teks dengan angka  yang akan dihapus.\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menghapus angka dari teks\n",
    "def hapus_angka(teks):\n",
    "    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])\n",
    "    return teks_tanpa_angka\n",
    " \n",
    "# Contoh teks dengan angka\n",
    "teks_dengan_angka = \"Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\"\n",
    " \n",
    "# Memanggil fungsi untuk menghapus angka\n",
    "teks_tanpa_angka = hapus_angka(teks_dengan_angka)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks dengan angka:\", teks_dengan_angka)\n",
    "print(\"Teks tanpa angka:\", teks_tanpa_angka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b1ad6",
   "metadata": {},
   "source": [
    "Inilah contoh penggunaan Regex untuk menghapus angka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f62327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat dengan angka: Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut.\n",
      "Kalimat tanpa angka tidak relevan: Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi  untuk informasi lebih lanjut.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "def hapus_angka_tidak_relevan(teks):\n",
    "    # Menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan\n",
    "    # Pola untuk mengenali angka yang harus dihapus, termasuk nomor rumah dan nomor telepon\n",
    "    pola_angka_tidak_relevan = r\"\\b(?:\\d{1,3}[-\\.\\s]?)?(?:\\d{3}[-\\.\\s]?)?\\d{4,}\\b\"\n",
    "    hasil = re.sub(pola_angka_tidak_relevan, \"\", teks)\n",
    "    return hasil.strip()\n",
    " \n",
    "# Contoh kalimat dengan angka\n",
    "kalimat = \"Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut.\"\n",
    " \n",
    "# Memanggil fungsi untuk menghapus angka tidak relevan\n",
    "hasil_tanpa_angka = hapus_angka_tidak_relevan(kalimat)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Kalimat dengan angka:\", kalimat)\n",
    "print(\"Kalimat tanpa angka tidak relevan:\", hasil_tanpa_angka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ffb16",
   "metadata": {},
   "source": [
    "Ekspresi regex ini memiliki beberapa komponen penting sebagai berikut:\n",
    "\n",
    "- `\\b` → **anchor word boundary** yang menandakan batas antara karakter *word* (kata) dan *non-word* (bukan kata).  \n",
    "  Ini memastikan bahwa kita hanya mencocokkan angka yang muncul sebagai kata terpisah.\n",
    "\n",
    "- `\\d+` → pola regex yang mencocokkan **satu atau lebih digit (angka)**.\n",
    "\n",
    "- `\\b` → **anchor word boundary** lagi yang menutup pola,  \n",
    "  memastikan bahwa angka yang dicocokkan adalah kata tunggal yang berdiri sendiri.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94696a",
   "metadata": {},
   "source": [
    "### Menghapus Tanda Baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fe6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\n",
      "Teks setelah menghapus tanda baca: Ini adalah contoh teks dengan tanda baca Contoh ini digunakan untuk demonstrasi\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    " \n",
    "def remove_punctuation(text):\n",
    "    # Membuat set yang berisi semua tanda baca\n",
    "    punctuation_set = set(string.punctuation)\n",
    " \n",
    "    # Menghapus tanda baca dari teks\n",
    "    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
    " \n",
    "    return text_without_punctuation\n",
    " \n",
    "# Contoh teks dengan tanda baca\n",
    "teks_asli = \"Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\"\n",
    " \n",
    "# Menghapus tanda baca dari teks\n",
    "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\", teks_asli)\n",
    "print(\"Teks setelah menghapus tanda baca:\", teks_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eedf091",
   "metadata": {},
   "source": [
    "### Menghapus Whitespace dalam Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c14370",
   "metadata": {},
   "source": [
    "Whitespace (karakter kosong) merujuk kepada karakter yang tidak terlihat pada layar, seperti spasi, tab, newline, dan karakter kosong lainnya. Karakter whitespace umumnya digunakan untuk memisahkan kata-kata atau elemen dalam teks, tetapi kadang-kadang mereka dapat muncul secara tidak diinginkan di awal, akhir, atau pertengahan teks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cbb85",
   "metadata": {},
   "source": [
    "### Menggunakan strip() untuk Menghapus Whitespace di Awal dan Akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964e4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah contoh kalimat dengan spasi di awal dan akhir.\n"
     ]
    }
   ],
   "source": [
    "teks = \"   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    \"\n",
    "teks_setelah_strip = teks.strip()\n",
    "print(teks_setelah_strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597d8da",
   "metadata": {},
   "source": [
    "### Menggunakan replace() untuk Menghapus Whitespace di Seluruh String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ebf451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniadalahcontohkalimatdenganspasididalamnya.\n"
     ]
    }
   ],
   "source": [
    "teks_dengan_whitespace = \"Ini adalah    contoh kalimat    dengan spasi    di dalamnya.\"\n",
    "teks_tanpa_whitespace = teks_dengan_whitespace.replace(\" \", \"\")\n",
    "print(teks_tanpa_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a524e",
   "metadata": {},
   "source": [
    "## Stopword Removal (Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c713b5",
   "metadata": {},
   "source": [
    "Penghapusan stopwords adalah langkah penting dalam pra-pemrosesan teks pada NLP. Stopwords adalah kata-kata umum yang sering muncul pada teks, tetapi tidak memiliki nilai informatif tinggi dalam analisis teks. Contohnya adalah kata-kata seperti \"dan\", \"di\", \"ke\", \"yang\", dan sebagainya. Tujuan penghapusan stopwords adalah membersihkan teks dari kata-kata umum tersebut sehingga fokus analisis dapat lebih pada kata-kata kunci yang lebih bermakna. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fb4c5",
   "metadata": {},
   "source": [
    "### Stopwords NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad2817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b418c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/ayam/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  # Untuk tokenisasi kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd14e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords NLTK: Perekonomian Indonesia pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "source": [
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    " \n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    " \n",
    "# Ambil daftar stopwords bahasa Indonesia dari NLTK\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    " \n",
    "# Filtering kata-kata dengan menghapus stopwords\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]\n",
    " \n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    " \n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords NLTK:\", teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb8df7",
   "metadata": {},
   "source": [
    "### Stopwords Sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee745e",
   "metadata": {},
   "source": [
    "Sastrawi adalah pustaka Python yang fokus pada pemrosesan teks dalam bahasa Indonesia, terutama untuk NLP. Sastrawi menyediakan daftar stopwords yang dioptimalkan untuk bahasa Indonesia sehingga sangat cocok digunakan pada analisis teks dalam bahasa Indonesia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "608c5d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords Sastrawi: Perekonomian Indonesia sedang pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# Inisialisasi objek StopWordRemover dari Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    " \n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    " \n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    " \n",
    "# Filtering kata-kata dengan menghapus stopwords Sastrawi\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
    " \n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    " \n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords Sastrawi:\", teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e97ef",
   "metadata": {},
   "source": [
    "### Perbedaan Utama dari NLTK ama Sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51718a30",
   "metadata": {},
   "source": [
    "- **Bahasa yang Didukung**  \n",
    "  - **NLTK**: menyediakan stopwords untuk beberapa bahasa, termasuk bahasa Indonesia.  \n",
    "  - **Sastrawi**: dioptimalkan khusus untuk pemrosesan teks dalam bahasa Indonesia.  \n",
    "\n",
    "- **Kustomisasi**  \n",
    "  - **NLTK**: memiliki koleksi stopwords pada berbagai bahasa, sifatnya lebih umum.  \n",
    "  - **Sastrawi**: menyediakan daftar stopwords yang dioptimalkan khusus untuk bahasa Indonesia.  \n",
    "\n",
    "- **Pemilihan Tools**  \n",
    "  - **Sastrawi** lebih spesifik dan sesuai jika fokus pada teks **bahasa Indonesia**.  \n",
    "  - **NLTK** lebih cocok jika butuh stopwords untuk **berbagai bahasa** atau jika sudah memakai NLTK untuk pipeline NLP lain.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5f262",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bd04c",
   "metadata": {},
   "source": [
    "Tokenisasi (tokenizing) adalah proses membagi teks menjadi potongan-potongan lebih kecil yang disebut token. Token dapat berupa kata, frasa, atau entitas lain yang lebih kecil dari teks yang dianalisis. Tujuan tokenisasi adalah memecah teks menjadi unit-unit yang lebih mudah diolah atau diinterpretasi dalam analisis teks atau pemrosesan bahasa alami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc38efb",
   "metadata": {},
   "source": [
    "### Tokenisasi Kata (Word Tokenization)\n",
    "Memecah teks menjadi kata-kata individu. Biasanya, pemisah antar kata adalah spasi atau karakter whitespace lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ad9466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ae073",
   "metadata": {},
   "source": [
    "### Tokenisasi Kalimat (Sentence Tokenization)\n",
    "Memecah teks menjadi kalimat-kalimat. Pemisah antar kalimat dapat berupa tanda baca, seperti titik, tanda tanya, atau tanda seru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27890e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini adalah contoh tokenisasi kalimat.', 'Apakah ini kalimat kedua?', 'Ya, ini kalimat ketiga!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "text = \"Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1a355",
   "metadata": {},
   "source": [
    "### Tokenisasi Frasa (Phrase Tokenization)\n",
    "Memecah teks menjadi frasa-frasa atau unit-unit frasa yang lebih besar dari kata tunggal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88171b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pemrosesan', 'teks', 'adalah', 'cabang', 'ilmu', 'komputer', 'yang', 'berfokus', 'pada', 'pengolahan', 'teks', 'dan', 'dokumen', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "phrases = tokenizer.tokenize(text)\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63730aa",
   "metadata": {},
   "source": [
    "### Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)\n",
    "Menggunakan aturan linguistik atau aturan pemrosesan teks untuk membagi teks menjadi token. Contohnya termasuk tokenisasi khusus untuk URL, email, atau entitas tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4194db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan.\"\n",
    "tokens = re.findall(r'\\w+|\\d+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f17559",
   "metadata": {},
   "source": [
    "### Tokenisasi Berdasarkan Model (Model-based Tokenization)\n",
    "\n",
    "Menggunakan model statistik atau model bahasa untuk memprediksi token dalam teks. Contohnya termasuk tokenisasi berbasis mesin pembelajaran, seperti model pembelajaran berbasis Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad2fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']\n"
     ]
    }
   ],
   "source": [
    "# Misalnya menggunakan spasi sebagai pemisah kata\n",
    "text = \"Ini adalah contoh tokenisasi berbasis model.\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388a0f2",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c6409",
   "metadata": {},
   "source": [
    "Stemming adalah sebuah proses dalam bidang pemrosesan teks yang digunakan untuk menyederhanakan kata-kata ke bentuk dasarnya dengan cara menghilangkan awal ataupun akhiran kata. Tujuannya adalah agar kata-kata berbeda, tetapi berasal dari satu kata dasar, dapat direduksi menjadi bentuk yang lebih seragam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0ed51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli: running, Kata setelah stemming: run\n",
      "Kata asli: runs, Kata setelah stemming: run\n",
      "Kata asli: runner, Kata setelah stemming: runner\n",
      "Kata asli: ran, Kata setelah stemming: ran\n",
      "Kata asli: easily, Kata setelah stemming: easili\n",
      "Kata asli: fairness, Kata setelah stemming: fair\n",
      "Kata asli: better, Kata setelah stemming: better\n",
      "Kata asli: best, Kata setelah stemming: best\n",
      "Kata asli: cats, Kata setelah stemming: cat\n",
      "Kata asli: cacti, Kata setelah stemming: cacti\n",
      "Kata asli: geese, Kata setelah stemming: gees\n",
      "Kata asli: rocks, Kata setelah stemming: rock\n",
      "Kata asli: oxen, Kata setelah stemming: oxen\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "# Inisialisasi stemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Kata-kata asli\n",
    "words = [\"running\", \"runs\", \"runner\", \"ran\", \"easily\", \"fairness\", \"better\", \"best\", \"cats\", \"cacti\", \"geese\", \"rocks\", \"oxen\"]\n",
    " \n",
    "# Melakukan stemming pada setiap kata\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(f\"Kata asli: {word}, Kata setelah stemming: {stemmed_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd620287",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff152a1f",
   "metadata": {},
   "source": [
    "Lemmatization adalah sebuah teknik dalam pemrosesan teks untuk mengubah kata-kata ke bentuk dasar mereka, yang disebut lemma (atau lema). Tujuannya adalah menyatukan kata-kata dengan akar sama agar dapat direpresentasikan melalui satu bentuk dasar yang merepresentasikan makna kata tersebut secara tepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8a8adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ayam/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli: Run, Kata setelah lematisasi: run\n",
      "Kata asli: Cat, Kata setelah lematisasi: cat\n",
      "Kata asli: Good, Kata setelah lematisasi: good\n",
      "Kata asli: Goose, Kata setelah lematisasi: goose\n",
      "Kata asli: Rock, Kata setelah lematisasi: rock\n",
      "Kata asli: City, Kata setelah lematisasi: city\n",
      "Kata asli: Big, Kata setelah lematisasi: big\n",
      "Kata asli: Happy, Kata setelah lematisasi: happy\n",
      "Kata asli: Run, Kata setelah lematisasi: run\n",
      "Kata asli: Sleep, Kata setelah lematisasi: sleep\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Download wordnet jika belum di-download\n",
    "nltk.download('wordnet')\n",
    " \n",
    "# Inisialisasi lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Kata-kata asli\n",
    "words = [\"Run\", \"Cat\", \"Good\", \"Goose\", \"Rock\", \"City\", \"Big\", \"Happy\", \"Run\", \"Sleep\"]\n",
    " \n",
    "# Melakukan lematisasi pada setiap kata\n",
    "for word in words:\n",
    "    lemma_word = lemmatizer.lemmatize(word.lower())  # Mengonversi ke huruf kecil untuk memastikan pemrosesan yang konsisten\n",
    "    print(f\"Kata asli: {word}, Kata setelah lematisasi: {lemma_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6726c36",
   "metadata": {},
   "source": [
    "# Latihan Pra-pemrosesan Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fda322",
   "metadata": {},
   "source": [
    "## Pendahuluan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da42ba8",
   "metadata": {},
   "source": [
    "Pra-pemrosesan teks adalah tahap penting dalam analisis teks dan natural language processing (NLP). Tujuannya adalah membersihkan dan mempersiapkan teks mentah agar dapat diolah lebih lanjut dengan algoritma pemrosesan teks atau analisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f632b",
   "metadata": {},
   "source": [
    "### 1. Case Folding\n",
    "Proses mengubah semua huruf dalam teks menjadi huruf kecil atau huruf besar agar konsisten.  \n",
    "Contoh: `\"TeKS\"` → `\"teks\"` atau `\"TEKS\"`.\n",
    "\n",
    "### 2. Removal Special Characters\n",
    "Menghapus karakter khusus atau simbol yang tidak relevan dari teks.\n",
    "\n",
    "- **Menghapus Angka**: Menghilangkan semua angka dari teks.  \n",
    "- **Menghapus Tanda Baca**: Menghapus semua tanda baca dari teks.  \n",
    "- **Menghapus White Space**: Menghapus spasi tambahan atau karakter spasi ganda dari teks.  \n",
    "  - **Menggunakan `strip()`**: Menghapus spasi di awal dan akhir teks.  \n",
    "  - **Menggunakan `replace()`**: Mengganti spasi tambahan dengan string kosong di seluruh teks.  \n",
    "\n",
    "### 3. Stopword Removal (Filtering)\n",
    "Menghapus kata-kata yang umumnya tidak memberikan nilai tambah dalam analisis teks, seperti `\"dan\"`, `\"atau\"`, `\"yang\"`, dll.\n",
    "\n",
    "- **Stopword NLTK**: Menggunakan koleksi kata-kata stopword yang disediakan oleh NLTK.  \n",
    "- **Stopword Sastrawi**: Menghapus kata-kata stopword menggunakan kamus stopword dari Sastrawi (fokus Bahasa Indonesia).  \n",
    "\n",
    "### 4. Tokenizing\n",
    "Proses membagi teks menjadi bagian-bagian lebih kecil yang disebut token.\n",
    "\n",
    "- **Tokenisasi Kata (Word Tokenization)**: Memecah teks menjadi kata-kata individual.  \n",
    "- **Tokenisasi Kalimat (Sentence Tokenization)**: Memecah teks menjadi kalimat-kalimat.  \n",
    "- **Tokenisasi Frasa (Phrase Tokenization)**: Memecah teks menjadi frasa atau unit tertentu.  \n",
    "- **Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)**: Memecah teks berdasarkan aturan tertentu, misal tanda baca.  \n",
    "- **Tokenisasi Berdasarkan Model (Model-based Tokenization)**: Memecah teks menggunakan model linguistik atau machine learning.  \n",
    "\n",
    "### 5. Stemming\n",
    "Proses menghapus imbuhan dari kata untuk mengembalikannya ke bentuk dasar.  \n",
    "Contoh: `\"berlari\"`, `\"berlarian\"`, `\"lari\"` → `\"lar\"`.\n",
    "\n",
    "### 6. Lemmatization\n",
    "Proses mengubah kata ke bentuk dasarnya (lema) dengan mempertimbangkan konteks dan struktur bahasa.  \n",
    "Contoh: `\"menyanyikan\"` → `\"nyanyi\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147dfaab",
   "metadata": {},
   "source": [
    "# Latihan Ekstraksi Fitur pada Teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c5513",
   "metadata": {},
   "source": [
    "Berikut adalah beberapa teknik yang sering digunakan untuk ekstraksi fitur pada teks:\n",
    "\n",
    "- **Word Embedding**  \n",
    "  Mengubah kata menjadi vektor numerik yang membawa makna semantik.\n",
    "\n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF)**  \n",
    "  Mengukur frekuensi kata dalam dokumen relatif terhadap seberapa umum kata itu muncul di seluruh dokumen.\n",
    "\n",
    "- **Bag of Words (BoW)**  \n",
    "  Merepresentasikan teks sebagai vektor frekuensi kata tanpa memperhatikan urutan kata.\n",
    "\n",
    "- **N-gram**  \n",
    "  Mengambil urutan kata/kombinasi kata sebagai fitur, misal bigram (dua kata berurutan) atau trigram (tiga kata berurutan).\n",
    "\n",
    "- **POS Tagging (Part of Speech Tagging)**  \n",
    "  Memberikan informasi jenis kata (kata benda, kata kerja, kata sifat, dll.) yang dapat menjadi fitur.\n",
    "\n",
    "- **Entity Recognition**  \n",
    "  Mengidentifikasi entitas penting dalam teks, seperti nama orang, lokasi, organisasi, tanggal, dll.\n",
    "\n",
    "- **Pola atau Pola Kata (Pattern Matching)**  \n",
    "  Mendeteksi pola tertentu dalam teks, misal ekspresi reguler untuk nomor telepon, email, atau kata kunci tertentu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0fbaa",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "Bayangkan Anda lagi main puzzle kata-kata. Anda punya sejumlah kata-kata yang tersusun dalam kalimat-kalimat, tetapi bagaimana cara mesin memahami makna sebenarnya dari kata-kata ini?\n",
    "\n",
    "Nah, di situlah \"word embedding\" masuk ke permainan. Hal ini seperti memberi setiap kata-kata tempat spesial dalam ruang matematis yang besar. Bayangkan ruang itu seperti tempat parkir yang luas dan setiap kata punya tempat parkirnya sendiri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbfb6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 19:14:46.324447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', 'clang 18'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.5.1'), ('cudnn_version', '9'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.sysconfig.get_build_info())\n",
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d1d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c5e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ayam/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc7d9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    'Saya suka makan bakso',\n",
    "    'Bakso enak dan lezat',\n",
    "    'Makanan favorit saya adalah nasi goreng',\n",
    "    'Nasi goreng pedas adalah makanan favorit saya',\n",
    "    'Saya suka makanan manis seperti es krim',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f356c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a64571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051171c",
   "metadata": {},
   "source": [
    "Berikut adalah penjelasan parameter yang digunakan:\n",
    "\n",
    "- **sentences**  \n",
    "  Data teks yang telah di-tokenisasi, biasanya berupa list of list kata.\n",
    "\n",
    "- **vector_size**  \n",
    "  Ukuran dari vektor representasi kata. Contoh: 100 berarti setiap kata direpresentasikan sebagai vektor 100 dimensi.\n",
    "\n",
    "- **window**  \n",
    "  Jumlah maksimum kata yang dianggap sebagai konteks dekat dalam satu kalimat.  \n",
    "  Contoh: `window=5` berarti 5 kata sebelum dan sesudah kata target dipertimbangkan sebagai konteks.\n",
    "\n",
    "- **min_count**  \n",
    "  Jumlah minimum kemunculan sebuah kata dalam korpus agar kata tersebut diperhitungkan dalam model.  \n",
    "  Contoh: `min_count=2` berarti kata yang muncul kurang dari 2 kali akan diabaikan.\n",
    "\n",
    "- **workers**  \n",
    "  Jumlah thread yang digunakan dalam proses pembangunan model. Mempercepat training pada mesin multi-core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9ab224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata-kata yang mirip dengan 'bakso': [('manis', 0.2529163062572479), ('nasi', 0.17018669843673706), ('enak', 0.15006467700004578)]\n",
      "Vektor untuk 'bakso': [-0.00713882  0.00124156 -0.00717766 -0.00224369  0.00371885  0.00583258\n",
      "  0.00119832  0.00210183 -0.00411138  0.00722588 -0.00630644  0.00464789\n",
      " -0.00821918  0.00203677 -0.00497649 -0.00424685 -0.00310906  0.00565491\n",
      "  0.00579776 -0.00497439  0.00077378 -0.0084959   0.00780977  0.00925648\n",
      " -0.00274235  0.0007995   0.00074748  0.00547704 -0.00860589  0.00058358\n",
      "  0.00687019  0.00223141  0.00112457 -0.00932216  0.00848288 -0.0062632\n",
      " -0.00299165  0.00349458 -0.00077282  0.00141124  0.00178217 -0.00682961\n",
      " -0.00972456  0.00904072  0.00619895 -0.00691193  0.00340259  0.00020664\n",
      "  0.00475438 -0.00712046  0.00402629  0.00434812  0.00995727 -0.00447314\n",
      " -0.00138943 -0.00731689 -0.00969748 -0.00908048 -0.00102362 -0.00650396\n",
      "  0.0048507  -0.00616346  0.0025184   0.00073924 -0.00339173 -0.00097928\n",
      "  0.00997817  0.009146   -0.00446089  0.00908287 -0.00564239  0.00593029\n",
      " -0.00309763  0.00343232  0.00301726  0.00690047 -0.00237434  0.00877584\n",
      "  0.00759023 -0.00954767 -0.00800735 -0.00763848  0.0029233  -0.00279572\n",
      " -0.00692899 -0.00812822  0.0083084   0.0019909  -0.00932751 -0.00479288\n",
      "  0.00313647 -0.00471295  0.0052802  -0.00423267  0.00264146 -0.00804574\n",
      "  0.00620901  0.00481829  0.00078651  0.00301266]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    " \n",
    "similar_words = word_vectors.most_similar('bakso', topn=3)\n",
    "print(\"Kata-kata yang mirip dengan 'bakso':\", similar_words)\n",
    " \n",
    "vector = word_vectors['bakso']\n",
    "print(\"Vektor untuk 'bakso':\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf56a8",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Bayangkan Anda memiliki sejumlah dokumen teks, seperti artikel, laporan, atau pesan-pesan dalam media sosial. Anda ingin tahu kata-kata yang penting dalam setiap dokumen itu. Misalnya, dalam artikel tentang makanan, kata-kata seperti \"makanan\" atau \"resep\" mungkin lebih penting daripada kata-kata umum lainnya.\n",
    "\n",
    "Di sinilah TF-IDF (Term Frequency-Inverse Document Frequency) masuk permainan. Ini seperti memberi bobot kepada setiap kata dalam dokumen berdasarkan seberapa sering kata itu muncul pada dokumen itu sendiri (frekuensi kata) dan seberapa umumnya kata itu muncul di seluruh kumpulan dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb7c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968a48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Saya suka makan bakso\",\n",
    "    \"Bakso enak dan lezat\",\n",
    "    \"Makanan favorit saya adalah nasi goreng\",\n",
    "    \"Nasi goreng pedas adalah makanan favorit saya\",\n",
    "    \"Saya suka makanan manis seperti es krim\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941d2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e456d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e81e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e988f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.49851188 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.61789262 0.         0.\n",
      "  0.         0.         0.34810993 0.         0.49851188]\n",
      " [0.         0.42224214 0.52335825 0.52335825 0.         0.\n",
      "  0.         0.         0.52335825 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.43951606 0.         0.         0.         0.         0.43951606\n",
      "  0.43951606 0.         0.         0.         0.36483803 0.\n",
      "  0.43951606 0.         0.30691325 0.         0.        ]\n",
      " [0.38596041 0.         0.         0.         0.         0.38596041\n",
      "  0.38596041 0.         0.         0.         0.320382   0.\n",
      "  0.38596041 0.47838798 0.26951544 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.42966246 0.\n",
      "  0.         0.42966246 0.         0.         0.28774996 0.42966246\n",
      "  0.         0.         0.24206433 0.42966246 0.34664897]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600622d8",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "Bag of Words (BoW) adalah pendekatan sederhana dalam pemrosesan teks yang mengubah teks menjadi representasi numerik. Ide dasarnya adalah kita menganggap setiap dokumen sebagai \"tas\" (bag) kata-kata dan hanya peduli tentang keberadaan kata-kata dalam dokumen tersebut, bukan urutan atau konteksnya. Kemudian, untuk setiap dokumen, kita hitung berapa kali setiap kata muncul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dac6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c71d9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Ini adalah contoh dokumen pertama.\",\n",
    "    \"Ini adalah dokumen kedua.\",\n",
    "    \"Ini adalah dokumen ketiga.\",\n",
    "    \"Ini adalah contoh contoh contoh.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5665184",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ac2be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04f03094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 3, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0faa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5749c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriks BoW:\n",
      "[[1 1 1 1 0 0 1]\n",
      " [1 0 1 1 1 0 0]\n",
      " [1 0 1 1 0 1 0]\n",
      " [1 3 0 1 0 0 0]]\n",
      "\n",
      "Daftar Fitur:\n",
      "['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriks BoW:\")\n",
    "print(bow_matrix.toarray())\n",
    " \n",
    "print(\"\\nDaftar Fitur:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d67a6",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc74a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b10b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Saya suka makan bakso enak di warung dekat rumah.\",\n",
    "    \"Nasi goreng adalah salah satu makanan favorit saya.\",\n",
    "    \"Es krim coklat sangat lezat dan menyegarkan.\",\n",
    "    \"Saat hari hujan, saya suka minum teh hangat.\",\n",
    "    \"Pemandangan pegunungan di pagi hari sangat indah.\",\n",
    "    \"Bola basket adalah olahraga favorit saya sejak kecil.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4cc5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    unigrams = list(ngrams(words, 1))\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    trigrams = list(ngrams(words, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52b862d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kalimat: Bola basket adalah olahraga favorit saya sejak kecil.\n",
      "1-gram:\n",
      "('Bola',)\n",
      "('basket',)\n",
      "('adalah',)\n",
      "('olahraga',)\n",
      "('favorit',)\n",
      "('saya',)\n",
      "('sejak',)\n",
      "('kecil.',)\n",
      "\n",
      "2-gram:\n",
      "('Bola', 'basket')\n",
      "('basket', 'adalah')\n",
      "('adalah', 'olahraga')\n",
      "('olahraga', 'favorit')\n",
      "('favorit', 'saya')\n",
      "('saya', 'sejak')\n",
      "('sejak', 'kecil.')\n",
      "\n",
      "3-gram:\n",
      "('Bola', 'basket', 'adalah')\n",
      "('basket', 'adalah', 'olahraga')\n",
      "('adalah', 'olahraga', 'favorit')\n",
      "('olahraga', 'favorit', 'saya')\n",
      "('favorit', 'saya', 'sejak')\n",
      "('saya', 'sejak', 'kecil.')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nKalimat:\", sentence)\n",
    "print(\"1-gram:\")\n",
    "for gram in unigrams:\n",
    "    print(gram)\n",
    "print(\"\\n2-gram:\")\n",
    "for gram in bigrams:\n",
    "    print(gram)\n",
    "print(\"\\n3-gram:\")\n",
    "for gram in trigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3db74",
   "metadata": {},
   "source": [
    "# Studi Kasus Implementasi Klasifikasi Teks pada NLP: Sentimen Analisis Review APK Play Store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
